{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.is_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, = self.params\n",
    "        out = np.dot(x, W)\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        self.grads[0][...] = dW\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        W, b = self.params\n",
    "        out = np.dot(x, W) + b\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        W, b = self.params\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dW = np.dot(self.x.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None  # softmaxの出力\n",
    "        self.t = None  # 教師ラベル\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "\n",
    "        # 教師ラベルがone-hotベクトルの場合、正解のインデックスに変換\n",
    "        if self.t.size == self.y.size:\n",
    "            self.t = self.t.argmax(axis=1)\n",
    "\n",
    "        loss = cross_entropy_error(self.y, self.t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = self.y.copy()\n",
    "        dx[np.arange(batch_size), self.t] -= 1\n",
    "        dx *= dout\n",
    "        dx = dx / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoidの出力\n",
    "        self.t = None  # 教師データ\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    '''\n",
    "    http://arxiv.org/abs/1207.0580\n",
    "    '''\n",
    "    def __init__(self, dropout_ratio=0.5):\n",
    "        self.params, self.grads = [], []\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.idx = None\n",
    "\n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0\n",
    "#         if GPU:\n",
    "#             np.scatter_add(dW, self.idx, dout)\n",
    "#         else:\n",
    "#             np.add.at(dW, self.idx, dout)\n",
    "\n",
    "        np.add.at(dW, self.idx, dout)\n",
    "\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)\n",
    "        out = np.sum(target_W * h, axis=1)\n",
    "\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "\n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "#         if not GPU:\n",
    "#             negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "#             for i in range(batch_size):\n",
    "#                 p = self.word_p.copy()\n",
    "#                 target_idx = target[i]\n",
    "#                 p[target_idx] = 0\n",
    "#                 p /= p.sum()\n",
    "#                 negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "#         else:\n",
    "#             # GPU(cupy）で計算するときは、速度を優先\n",
    "#             # 負例にターゲットが含まれるケースがある\n",
    "#             negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "#                                                replace=True, p=self.word_p)\n",
    "\n",
    "        negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            p = self.word_p.copy()\n",
    "            target_idx = target[i]\n",
    "            p[target_idx] = 0\n",
    "            p /= p.sum()\n",
    "            negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss:\n",
    "    def __init__(self, W, corpus, power=0.75, sample_size=5):\n",
    "        self.sample_size = sample_size\n",
    "        self.sampler = UnigramSampler(corpus, power, sample_size)\n",
    "        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]\n",
    "        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.embed_dot_layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, h, target):\n",
    "        batch_size = target.shape[0]\n",
    "        negative_sample = self.sampler.get_negative_sample(target)\n",
    "\n",
    "        # 正例のフォワード\n",
    "        score = self.embed_dot_layers[0].forward(h, target)\n",
    "        correct_label = np.ones(batch_size, dtype=np.int32)\n",
    "        loss = self.loss_layers[0].forward(score, correct_label)\n",
    "\n",
    "        # 負例のフォワード\n",
    "        negative_label = np.zeros(batch_size, dtype=np.int32)\n",
    "        for i in range(self.sample_size):\n",
    "            negative_target = negative_sample[:, i]\n",
    "            score = self.embed_dot_layers[1 + i].forward(h, negative_target)\n",
    "            loss += self.loss_layers[1 + i].forward(score, negative_label)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dh = 0\n",
    "        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):\n",
    "            dscore = l0.backward(dout)\n",
    "            dh += l1.backward(dscore)\n",
    "\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "            dxs[:, t, :] = dx\n",
    "\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "\n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        out = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            out[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "\n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "\n",
    "        self.grads[0][...] = grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "\n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "\n",
    "        if ts.ndim == 3:  # 教師ラベルがone-hotベクトルの場合\n",
    "            ts = ts.argmax(axis=2)\n",
    "\n",
    "        mask = (ts != self.ignore_label)\n",
    "\n",
    "        # バッチ分と時系列分をまとめる（reshape）\n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "\n",
    "        ys = softmax(xs)\n",
    "        ls = np.log(ys[np.arange(N * T), ts])\n",
    "        ls *= mask  # ignore_labelに該当するデータは損失を0にする\n",
    "        loss = -np.sum(ls)\n",
    "        loss /= mask.sum()\n",
    "\n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "\n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx *= dout\n",
    "        dx /= mask.sum()\n",
    "        dx *= mask[:, np.newaxis]  # ignore_labelに該当するデータは勾配を0にする\n",
    "\n",
    "        dx = dx.reshape((N, T, V))\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: 入力`x`用の重みパラーメタ（4つ分の重みをまとめる）\n",
    "        Wh: 隠れ状態`h`用の重みパラメータ（4つ分の重みをまとめる）\n",
    "        b: バイアス（4つ分のバイアスをまとめる）\n",
    "        '''\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, H = h_prev.shape\n",
    "\n",
    "        A = np.dot(x, Wx) + np.dot(h_prev, Wh) + b\n",
    "\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "\n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "\n",
    "        c_next = f * c_prev + g * i\n",
    "        h_next = o * np.tanh(c_next)\n",
    "\n",
    "        self.cache = (x, h_prev, c_prev, i, f, g, o, c_next)\n",
    "        return h_next, c_next\n",
    "\n",
    "    def backward(self, dh_next, dc_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, c_prev, i, f, g, o, c_next = self.cache\n",
    "\n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "\n",
    "        ds = dc_next + (dh_next * o) * (1 - tanh_c_next ** 2)\n",
    "\n",
    "        dc_prev = ds * f\n",
    "\n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        do = dh_next * tanh_c_next\n",
    "        dg = ds * i\n",
    "\n",
    "        di *= i * (1 - i)\n",
    "        df *= f * (1 - f)\n",
    "        do *= o * (1 - o)\n",
    "        dg *= (1 - g ** 2)\n",
    "\n",
    "        dA = np.hstack((df, dg, di, do))\n",
    "\n",
    "        dWh = np.dot(h_prev.T, dA)\n",
    "        dWx = np.dot(x.T, dA)\n",
    "        db = dA.sum(axis=0)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        dx = np.dot(dA, Wx.T)\n",
    "        dh_prev = np.dot(dA, Wh.T)\n",
    "\n",
    "        return dx, dh_prev, dc_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeLSTM:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.c = None, None\n",
    "        self.dh = None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        H = Wh.shape[0]\n",
    "\n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "\n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "        if not self.stateful or self.c is None:\n",
    "            self.c = np.zeros((N, H), dtype='f')\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = LSTM(*self.params)\n",
    "            self.h, self.c = layer.forward(xs[:, t, :], self.h, self.c)\n",
    "            hs[:, t, :] = self.h\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D = Wx.shape[0]\n",
    "\n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh, dc = 0, 0\n",
    "\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh, dc = layer.backward(dhs[:, t, :] + dh, dc)\n",
    "            dxs[:, t, :] = dx\n",
    "            for i, grad in enumerate(layer.grads):\n",
    "                grads[i] += grad\n",
    "\n",
    "        for i, grad in enumerate(grads):\n",
    "            self.grads[i][...] = grad\n",
    "        self.dh = dh\n",
    "        return dxs\n",
    "\n",
    "    def set_state(self, h, c=None):\n",
    "        self.h, self.c = h, c\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.h, self.c = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    '''\n",
    "    確率的勾配降下法（Stochastic Gradient Descent）\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        for i in range(len(params)):\n",
    "            params[i] -= self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Momentum:\n",
    "    '''\n",
    "    Momentum SGD\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] = self.momentum * self.v[i] - self.lr * grads[i]\n",
    "            params[i] += self.v[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nesterov:\n",
    "    '''\n",
    "    Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = []\n",
    "            for param in params:\n",
    "                self.v.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.v[i] *= self.momentum\n",
    "            self.v[i] -= self.lr * grads[i]\n",
    "            params[i] += self.momentum * self.momentum * self.v[i]\n",
    "            params[i] -= (1 + self.momentum) * self.lr * grads[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    '''\n",
    "    AdaGrad\n",
    "    '''\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] += grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSprop:\n",
    "    '''\n",
    "    RMSprop\n",
    "    '''\n",
    "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = []\n",
    "            for param in params:\n",
    "                self.h.append(np.zeros_like(param))\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.h[i] *= self.decay_rate\n",
    "            self.h[i] += (1 - self.decay_rate) * grads[i] * grads[i]\n",
    "            params[i] -= self.lr * grads[i] / (np.sqrt(self.h[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_list = []\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=20):\n",
    "        data_size = len(x)\n",
    "        max_iters = data_size // batch_size\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            # シャッフル\n",
    "            idx = np.random.permutation(np.arange(data_size))\n",
    "            x = x[idx]\n",
    "            t = t[idx]\n",
    "\n",
    "            for iters in range(max_iters):\n",
    "                batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "                batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "                # 勾配を求め、パラメータを更新\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # 評価\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    avg_loss = total_loss / loss_count\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| epoch %d |  iter %d / %d | time %d[s] | loss %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))\n",
    "                    self.loss_list.append(float(avg_loss))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.loss_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.loss_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('loss')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnlmTrainer:\n",
    "    def __init__(self, model, optimizer):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.time_idx = None\n",
    "        self.ppl_list = None\n",
    "        self.eval_interval = None\n",
    "        self.current_epoch = 0\n",
    "\n",
    "    def get_batch(self, x, t, batch_size, time_size):\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "\n",
    "        data_size = len(x)\n",
    "        jump = data_size // batch_size\n",
    "        offsets = [i * jump for i in range(batch_size)]  # バッチの各サンプルの読み込み開始位置\n",
    "\n",
    "        for time in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, time] = x[(offset + self.time_idx) % data_size]\n",
    "                batch_t[i, time] = t[(offset + self.time_idx) % data_size]\n",
    "            self.time_idx += 1\n",
    "        return batch_x, batch_t\n",
    "\n",
    "    def fit(self, xs, ts, max_epoch=10, batch_size=20, time_size=35,\n",
    "            max_grad=None, eval_interval=20):\n",
    "        data_size = len(xs)\n",
    "        max_iters = data_size // (batch_size * time_size)\n",
    "        self.time_idx = 0\n",
    "        self.ppl_list = []\n",
    "        self.eval_interval = eval_interval\n",
    "        model, optimizer = self.model, self.optimizer\n",
    "        total_loss = 0\n",
    "        loss_count = 0\n",
    "\n",
    "        start_time = time.time()\n",
    "        for epoch in range(max_epoch):\n",
    "            for iters in range(max_iters):\n",
    "                batch_x, batch_t = self.get_batch(xs, ts, batch_size, time_size)\n",
    "\n",
    "                # 勾配を求め、パラメータを更新\n",
    "                loss = model.forward(batch_x, batch_t)\n",
    "                model.backward()\n",
    "                params, grads = remove_duplicate(model.params, model.grads)  # 共有された重みを1つに集約\n",
    "                if max_grad is not None:\n",
    "                    clip_grads(grads, max_grad)\n",
    "                optimizer.update(params, grads)\n",
    "                total_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "                # パープレキシティの評価\n",
    "                if (eval_interval is not None) and (iters % eval_interval) == 0:\n",
    "                    ppl = np.exp(total_loss / loss_count)\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    print('| epoch %d |  iter %d / %d | time %d[s] | perplexity %.2f'\n",
    "                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, ppl))\n",
    "                    self.ppl_list.append(float(ppl))\n",
    "                    total_loss, loss_count = 0, 0\n",
    "\n",
    "            self.current_epoch += 1\n",
    "\n",
    "    def plot(self, ylim=None):\n",
    "        x = np.arange(len(self.ppl_list))\n",
    "        if ylim is not None:\n",
    "            plt.ylim(*ylim)\n",
    "        plt.plot(x, self.ppl_list, label='train')\n",
    "        plt.xlabel('iterations (x' + str(self.eval_interval) + ')')\n",
    "        plt.ylabel('perplexity')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate(params, grads):\n",
    "    '''\n",
    "    パラメータ配列中の重複する重みをひとつに集約し、\n",
    "    その重みに対応する勾配を加算する\n",
    "    '''\n",
    "    params, grads = params[:], grads[:]  # copy list\n",
    "\n",
    "    while True:\n",
    "        find_flg = False\n",
    "        L = len(params)\n",
    "\n",
    "        for i in range(0, L - 1):\n",
    "            for j in range(i + 1, L):\n",
    "                # 重みを共有する場合\n",
    "                if params[i] is params[j]:\n",
    "                    grads[i] += grads[j]  # 勾配の加算\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "                # 転置行列として重みを共有する場合（weight tying）\n",
    "                elif params[i].ndim == 2 and params[j].ndim == 2 and \\\n",
    "                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):\n",
    "                    grads[i] += grads[j].T\n",
    "                    find_flg = True\n",
    "                    params.pop(j)\n",
    "                    grads.pop(j)\n",
    "\n",
    "                if find_flg: break\n",
    "            if find_flg: break\n",
    "\n",
    "        if not find_flg: break\n",
    "\n",
    "    return params, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    if rate < 1:\n",
    "        for grad in grads:\n",
    "            grad *= rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(words):\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    '''コサイン類似度の算出\n",
    "    :param x: ベクトル\n",
    "    :param y: ベクトル\n",
    "    :param eps: ”0割り”防止のための微小値\n",
    "    :return:\n",
    "    '''\n",
    "    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)\n",
    "    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''共起行列の作成\n",
    "    :param corpus: コーパス（単語IDのリスト）\n",
    "    :param vocab_size:語彙数\n",
    "    :param window_size:ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
    "    :return: 共起行列\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    '''類似単語の検索\n",
    "    :param query: クエリ（テキスト）\n",
    "    :param word_to_id: 単語から単語IDへのディクショナリ\n",
    "    :param id_to_word: 単語IDから単語へのディクショナリ\n",
    "    :param word_matrix: 単語ベクトルをまとめた行列。各行に対応する単語のベクトルが格納されていることを想定する\n",
    "    :param top: 上位何位まで表示するか\n",
    "    '''\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' % query)\n",
    "        return\n",
    "\n",
    "    print('\\n[query] ' + query)\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    vocab_size = len(id_to_word)\n",
    "\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(' %s: %s' % (id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(C, verbose=False, eps = 1e-8):\n",
    "    '''PPMI（正の相互情報量）の作成\n",
    "    :param C: 共起行列\n",
    "    :param verbose: 進行状況を出力するかどうか\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j]*S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total//100 + 1) == 0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_hot(corpus, vocab_size):\n",
    "    '''one-hot表現への変換\n",
    "    :param corpus: 単語IDのリスト（1次元もしくは2次元のNumPy配列）\n",
    "    :param vocab_size: 語彙数\n",
    "    :return: one-hot表現（2次元もしくは3次元のNumPy配列）\n",
    "    '''\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_contexts_target(corpus, window_size=1):\n",
    "    '''コンテキストとターゲットの作成\n",
    "    :param corpus: コーパス（単語IDのリスト）\n",
    "    :param window_size: ウィンドウサイズ（ウィンドウサイズが1のときは、単語の左右1単語がコンテキスト）\n",
    "    :return:\n",
    "    '''\n",
    "    target = corpus[window_size:-window_size]\n",
    "    contexts = []\n",
    "\n",
    "    for idx in range(window_size, len(corpus)-window_size):\n",
    "        cs = []\n",
    "        for t in range(-window_size, window_size + 1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx + t])\n",
    "        contexts.append(cs)\n",
    "\n",
    "    return np.array(contexts), np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):\n",
    "    for word in (a, b, c):\n",
    "        if word not in word_to_id:\n",
    "            print('%s is not found' % word)\n",
    "            return\n",
    "\n",
    "    print('\\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')\n",
    "    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]\n",
    "    query_vec = b_vec - a_vec + c_vec\n",
    "    query_vec = normalize(query_vec)\n",
    "\n",
    "    similarity = np.dot(word_matrix, query_vec)\n",
    "\n",
    "    if answer is not None:\n",
    "        print(\"==>\" + answer + \":\" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))\n",
    "\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if np.isnan(similarity[i]):\n",
    "            continue\n",
    "        if id_to_word[i] in (a, b, c):\n",
    "            continue\n",
    "        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def eval_perplexity(model, corpus, batch_size=10, time_size=35):\n",
    "    print('evaluating perplexity ...')\n",
    "    corpus_size = len(corpus)\n",
    "    total_loss, loss_cnt = 0, 0\n",
    "    max_iters = (corpus_size - 1) // (batch_size * time_size)\n",
    "    jump = (corpus_size - 1) // batch_size\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        xs = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        ts = np.zeros((batch_size, time_size), dtype=np.int32)\n",
    "        time_offset = iters * time_size\n",
    "        offsets = [time_offset + (i * jump) for i in range(batch_size)]\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                xs[i, t] = corpus[(offset + t) % corpus_size]\n",
    "                ts[i, t] = corpus[(offset + t + 1) % corpus_size]\n",
    "\n",
    "        try:\n",
    "            loss = model.forward(xs, ts, train_flg=False)\n",
    "        except TypeError:\n",
    "            loss = model.forward(xs, ts)\n",
    "        total_loss += loss\n",
    "\n",
    "        sys.stdout.write('\\r%d / %d' % (iters, max_iters))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print('')\n",
    "    ppl = np.exp(total_loss / max_iters)\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    if x.ndim == 2:\n",
    "        s = np.sqrt((x * x).sum(1))\n",
    "        x /= s.reshape((s.shape[0], 1))\n",
    "    elif x.ndim == 1:\n",
    "        s = np.sqrt((x * x).sum())\n",
    "        x /= s\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spiral data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from dataset import spiral\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "print('x', x.shape)  # (300, 2)\n",
    "print('t', t.shape)  # (300, 3)\n",
    "\n",
    "# データ点のプロット\n",
    "N = 100\n",
    "CLS_NUM = 3\n",
    "markers = ['o', 'x', '^']\n",
    "for i in range(CLS_NUM):\n",
    "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two layer net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I, H, O = input_size, hidden_size, output_size\n",
    "\n",
    "        # 重みとバイアスの初期化\n",
    "        W1 = 0.01 * np.random.randn(I, H)\n",
    "        b1 = np.zeros(H)\n",
    "        W2 = 0.01 * np.random.randn(H, O)\n",
    "        b2 = np.zeros(O)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            Affine(W1, b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        score = self.predict(x)\n",
    "        loss = self.loss_layer.forward(score, t)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ハイパーパラメータの設定\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "trainer = Trainer(model, optimizer)\n",
    "trainer.fit(x, t, max_epoch, batch_size, eval_interval=10)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ハイパーパラメータの設定\n",
    "max_epoch = 300\n",
    "batch_size = 30\n",
    "hidden_size = 10\n",
    "learning_rate = 1.0\n",
    "\n",
    "x, t = spiral.load_data()\n",
    "model = TwoLayerNet(input_size=2, hidden_size=hidden_size, output_size=3)\n",
    "optimizer = SGD(lr=learning_rate)\n",
    "\n",
    "# 学習で使用する変数\n",
    "data_size = len(x)\n",
    "max_iters = data_size // batch_size\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    # データのシャッフル\n",
    "    idx = np.random.permutation(data_size)\n",
    "    x = x[idx]\n",
    "    t = t[idx]\n",
    "\n",
    "    for iters in range(max_iters):\n",
    "        batch_x = x[iters*batch_size:(iters+1)*batch_size]\n",
    "        batch_t = t[iters*batch_size:(iters+1)*batch_size]\n",
    "\n",
    "        # 勾配を求め、パラメータを更新\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "\n",
    "        # 定期的に学習経過を出力\n",
    "        if (iters+1) % 10 == 0:\n",
    "            avg_loss = total_loss / loss_count\n",
    "            print('| epoch %d |  iter %d / %d | loss %.2f'\n",
    "                  % (epoch + 1, iters + 1, max_iters, avg_loss))\n",
    "            loss_list.append(avg_loss)\n",
    "            total_loss, loss_count = 0, 0\n",
    "\n",
    "\n",
    "# 学習結果のプロット\n",
    "plt.plot(np.arange(len(loss_list)), loss_list, label='train')\n",
    "plt.xlabel('iterations (x10)')\n",
    "plt.ylabel('loss')\n",
    "plt.show()\n",
    "\n",
    "# 境界領域のプロット\n",
    "h = 0.001\n",
    "x_min, x_max = x[:, 0].min() - .1, x[:, 0].max() + .1\n",
    "y_min, y_max = x[:, 1].min() - .1, x[:, 1].max() + .1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "X = np.c_[xx.ravel(), yy.ravel()]\n",
    "score = model.predict(X)\n",
    "predict_cls = np.argmax(score, axis=1)\n",
    "Z = predict_cls.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z)\n",
    "plt.axis('off')\n",
    "\n",
    "# データ点のプロット\n",
    "x, t = spiral.load_data()\n",
    "N = 100\n",
    "CLS_NUM = 3\n",
    "markers = ['o', 'x', '^']\n",
    "for i in range(CLS_NUM):\n",
    "    plt.scatter(x[i*N:(i+1)*N, 0], x[i*N:(i+1)*N, 1], s=40, marker=markers[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kurashiru データ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from dataset import kurashiru\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = kurashiru.load_recipes()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "ingredients_names = [pd.DataFrame(val)[\"name\"].to_list() for idx, val in df[\"ingredients\"].items()]\n",
    "\n",
    "words = reduce(lambda a, b: a + [\"<eos>\"] + b, ingredients_names)\n",
    "\n",
    "re_strip = re.compile('\\(.*\\)|（.*）|[①②③④⑤⑥⑦⑧⑨]')\n",
    "words = [re.sub(re_strip, '', w).strip() for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カウントベース"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ptbデータ"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from dataset import ptb\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "wordvec_size = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "print('counting  co-occurrence ...')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "print('calculating PPMI ...')\n",
    "W = ppmi(C, verbose=True)\n",
    "\n",
    "print('calculating SVD ...')\n",
    "try:\n",
    "    # truncated SVD (fast!)\n",
    "    print('sklearn')\n",
    "    from sklearn.utils.extmath import randomized_svd\n",
    "    U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5,\n",
    "                             random_state=None)\n",
    "except ImportError:\n",
    "    # SVD (slow)\n",
    "    print('numpy')\n",
    "    U, S, V = np.linalg.svd(W)\n",
    "\n",
    "# print('numpy')\n",
    "# U, S, V = np.linalg.svd(W)\n",
    "\n",
    "word_vecs = U[:, :wordvec_size]\n",
    "\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.set_printoptions(precision=3)  # 有効桁３桁で表示\n",
    "print(C[0])\n",
    "print(W[0])\n",
    "print(U[0])\n",
    "\n",
    "# plot\n",
    "for word, word_id in word_to_id.items():\n",
    "    plt.annotate(word, (U[word_id, 0], U[word_id, 1]))\n",
    "plt.scatter(U[:,0], U[:,1], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW (simple)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class SimpleCBOW:\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layer0 = MatMul(W_in)\n",
    "        self.in_layer1 = MatMul(W_in)\n",
    "        self.out_layer = MatMul(W_out)\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
    "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
    "        h = (h0 + h1) * 0.5\n",
    "        score = self.out_layer.forward(h)\n",
    "        loss = self.loss_layer.forward(score, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        ds = self.loss_layer.backward(dout)\n",
    "        da = self.out_layer.backward(ds)\n",
    "        da *= 0.5\n",
    "        self.in_layer1.backward(da)\n",
    "        self.in_layer0.backward(da)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "window_size = 1\n",
    "hidden_size = 5\n",
    "batch_size = 3\n",
    "max_epoch = 100\n",
    "\n",
    "corpus, word_to_id, id_to_word = preprocess(words)\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "target = convert_one_hot(target, vocab_size)\n",
    "contexts = convert_one_hot(contexts, vocab_size)\n",
    "\n",
    "model = SimpleCBOW(vocab_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# word_vecs = model.word_vecs\n",
    "# for word_id, word in id_to_word.items():\n",
    "#     print(word, word_vecs[word_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### skip-gram (simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, hidden_size, window_size, corpus):\n",
    "        V, H = vocab_size, hidden_size\n",
    "\n",
    "        # 重みの初期化\n",
    "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
    "        W_out = 0.01 * np.random.randn(V, H).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.in_layers = []\n",
    "        for i in range(2 * window_size):\n",
    "            layer = Embedding(W_in)  # Embeddingレイヤを使用\n",
    "            self.in_layers.append(layer)\n",
    "        self.ns_loss = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        layers = self.in_layers + [self.ns_loss]\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "        # メンバ変数に単語の分散表現を設定\n",
    "        self.word_vecs = W_in\n",
    "\n",
    "    def forward(self, contexts, target):\n",
    "        h = 0\n",
    "        for i, layer in enumerate(self.in_layers):\n",
    "            h += layer.forward(contexts[:, i])\n",
    "        h *= 1 / len(self.in_layers)\n",
    "        loss = self.ns_loss.forward(h, target)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.ns_loss.backward(dout)\n",
    "        dout *= 1 / len(self.in_layers)\n",
    "        for layer in self.in_layers:\n",
    "            layer.backward(dout)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from dataset import ptb\n",
    "import pickle\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "window_size = 5\n",
    "hidden_size = 100\n",
    "batch_size = 100\n",
    "max_epoch = 10\n",
    "\n",
    "# データの読み込み\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "vocab_size = len(word_to_id)\n",
    "\n",
    "contexts, target = create_contexts_target(corpus, window_size)\n",
    "# if config.GPU:\n",
    "#     contexts, target = to_gpu(contexts), to_gpu(target)\n",
    "\n",
    "# モデルなどの生成\n",
    "model = CBOW(vocab_size, hidden_size, window_size, corpus)\n",
    "# model = SkipGram(vocab_size, hidden_size, window_size, corpus)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "# 学習開始\n",
    "trainer.fit(contexts, target, max_epoch, batch_size)\n",
    "trainer.plot()\n",
    "\n",
    "# 後ほど利用できるように、必要なデータを保存\n",
    "# word_vecs = model.word_vecs\n",
    "# if config.GPU:\n",
    "#     word_vecs = to_cpu(word_vecs)\n",
    "params = {}\n",
    "params['word_vecs'] = word_vecs.astype(np.float16)\n",
    "params['word_to_id'] = word_to_id\n",
    "params['id_to_word'] = id_to_word\n",
    "pkl_file = 'cbow_params.pkl'  # or 'skipgram_params.pkl'\n",
    "with open(pkl_file, 'wb') as f:\n",
    "    pickle.dump(params, f, -1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pkl_file = 'cbow_params.pkl'\n",
    "# pkl_file = 'skipgram_params.pkl'\n",
    "\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    params = pickle.load(f)\n",
    "    word_vecs = params['word_vecs']\n",
    "    word_to_id = params['word_to_id']\n",
    "    id_to_word = params['id_to_word']\n",
    "\n",
    "# most similar task\n",
    "querys = ['you', 'year', 'car', 'toyota']\n",
    "for query in querys:\n",
    "    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)\n",
    "\n",
    "# analogy task\n",
    "print('-'*50)\n",
    "analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)\n",
    "analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN (Simple)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.rnn_layer.reset_state()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# ハイパーパラメータの設定\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNNの隠れ状態ベクトルの要素数\n",
    "time_size = 5  # RNNを展開するサイズ\n",
    "lr = 0.1\n",
    "max_epoch = 100\n",
    "\n",
    "# 学習データの読み込み\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000  # テスト用にデータセットを小さくする\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus) + 1)\n",
    "xs = corpus[:-1]  # 入力\n",
    "ts = corpus[1:]  # 出力（教師ラベル）\n",
    "\n",
    "# モデルの生成\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rnnlm:\n",
    "    def __init__(self, vocab_size=10000, wordvec_size=100, hidden_size=100):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        # 重みの初期化\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.lstm_layer = self.layers[1]\n",
    "\n",
    "        # すべての重みと勾配をリストにまとめる\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def predict(self, xs):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        return xs\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        score = self.predict(xs)\n",
    "        loss = self.loss_layer.forward(score, ts)\n",
    "        return loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.lstm_layer.reset_state()\n",
    "\n",
    "    def save_params(self, file_name='Rnnlm.pkl'):\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(self.params, f)\n",
    "\n",
    "    def load_params(self, file_name='Rnnlm.pkl'):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            self.params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 1 | time 2[s] | perplexity 9999.68\n",
      "| epoch 2 |  iter 1 / 1 | time 4[s] | perplexity 6827.26\n",
      "| epoch 3 |  iter 1 / 1 | time 6[s] | perplexity 2841.94\n",
      "| epoch 4 |  iter 1 / 1 | time 8[s] | perplexity 268792.89\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 480x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 345,
       "width": 447
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating perplexity ...\n",
      "234 / 235\n",
      "test perplexity:  6122.427246451025\n"
     ]
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "import pickle\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "batch_size = 20\n",
    "wordvec_size = 100\n",
    "hidden_size = 100  # RNNの隠れ状態ベクトルの要素数\n",
    "time_size = 35  # RNNを展開するサイズ\n",
    "lr = 20.0\n",
    "max_epoch = 4\n",
    "max_grad = 0.25\n",
    "\n",
    "# 学習データの読み込み\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "\n",
    "corpus_size = 1000  # テスト用にデータセットを小さくする\n",
    "corpus = corpus[:corpus_size]\n",
    "\n",
    "corpus_test, _, _ = ptb.load_data('test')\n",
    "vocab_size = len(word_to_id)\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "\n",
    "# モデルの生成\n",
    "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)\n",
    "trainer = RnnlmTrainer(model, optimizer)\n",
    "\n",
    "# 勾配クリッピングを適用して学習\n",
    "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\n",
    "            eval_interval=20)\n",
    "trainer.plot(ylim=(0, 500))\n",
    "\n",
    "# テストデータで評価\n",
    "model.reset_state()\n",
    "ppl_test = eval_perplexity(model, corpus_test)\n",
    "print('test perplexity: ', ppl_test)\n",
    "\n",
    "# パラメータの保存\n",
    "model.save_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
